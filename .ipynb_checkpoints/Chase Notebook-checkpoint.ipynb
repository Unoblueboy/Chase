{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, Input, optimizers\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from random import random, randint\n",
    "\n",
    "from ChaseEnvironment import ChaseEnv\n",
    "\n",
    "from ExperienceBuffers import ExperienceBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self, imsize):\n",
    "        \n",
    "        # Need to set up primary network\n",
    "        self.input_states = Input(shape=[imsize,imsize,3], dtype=tf.float32)\n",
    "        self.layer1 = layers.Conv2D(32, [8,8], strides = [4,4])(self.input_states)\n",
    "        self.layer2 = layers.Conv2D(64, [4,4], strides = [2,2])(self.layer1)\n",
    "        self.layer3 = layers.Conv2D(64, [3,3], strides = [1,1])(self.layer2)\n",
    "        self.layer4 = layers.Conv2D(64, [7,7], strides = [1,1])(self.layer3)\n",
    "        self.layer5 = layers.Flatten()(self.layer4)\n",
    "        self.layer6 = layers.Dense(32, activation='relu', \n",
    "                                      name='layer6')(self.layer5)\n",
    "#        self.layer4 = layers.Dense(32, activation='tanh', \n",
    "#                                      name='layer4')(self.layer3)\n",
    "#        self.layer5 = layers.Dense(32, activation='tanh', \n",
    "#                                      name='layer5')(self.layer4)\n",
    "        \n",
    "        \n",
    "        self.Value = layers.Dense(1, activation='linear', \n",
    "                                      name='value')(self.layer6)\n",
    "        self.Advantage = layers.Dense(4, activation='linear', \n",
    "                                      name='advantage')(self.layer6)\n",
    "        \n",
    "        def anon(lis):\n",
    "            adv = lis[1]\n",
    "            val = lis[0]\n",
    "            mean_adv = tf.reduce_mean(adv, axis = 1, keepdims = True)\n",
    "            return val+adv-mean_adv\n",
    "        \n",
    "        self.optimizer = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "        self.predict_Q = layers.Lambda(anon)([self.Value, self.Advantage])\n",
    "        self.model = tf.keras.Model(inputs=self.input_states, outputs=self.predict_Q)\n",
    "        self.model.compile(optimizer = self.optimizer,\n",
    "                           loss = 'mse')\n",
    "    \n",
    "    def act(self, state, batch_size=1):\n",
    "        q_values = self.model.predict(state, batch_size=batch_size)\n",
    "        return np.array(np.argmax(q_values, axis = 1), dtype = np.int32)\n",
    "    \n",
    "    def learn(self, states, q_values):\n",
    "        self.model.fit(states, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChaseEnv(5)\n",
    "\n",
    "# What up its ya boi, hyper parameters\n",
    "pre_train_steps = 512\n",
    "step_by_step_batch = False\n",
    "buffer_sample_size = 500\n",
    "epochs = 5\n",
    "batch_size = 20\n",
    "buffer_size = 5000\n",
    "# In terms of steps\n",
    "main_update_freq = 1\n",
    "# In terms of episodes \n",
    "target_update_freq = 50\n",
    "start_e = 1\n",
    "end_e = 0.01\n",
    "reducing_steps = 10000\n",
    "step_drop = (start_e - end_e)/reducing_steps\n",
    "discount = 0.95\n",
    "tau = 1\n",
    "save = True\n",
    "save_freq = 150\n",
    "train_player_1 = True\n",
    "train_player_2 = True\n",
    "rendering = True\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_player_1:\n",
    "    player_1_losses = []\n",
    "    player_1_main_q = DQN(env.imsize)\n",
    "    player_1_target_q = DQN(env.imsize)\n",
    "    player_1_target_q.model.set_weights(player_1_main_q.model.get_weights())\n",
    "\n",
    "if train_player_2:\n",
    "    player_2_losses = []\n",
    "    player_2_main_q = DQN(env.imsize)\n",
    "    player_2_target_q = DQN(env.imsize)\n",
    "    player_2_target_q.model.set_weights(player_2_main_q.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 Model Loaded\n",
      "Player 2 model loaded\n",
      "Models Loaded\n"
     ]
    }
   ],
   "source": [
    "if save:\n",
    "    try:\n",
    "        if train_player_1:\n",
    "            player_1_main_q.model = load_model('chase 1/player_1_main_q.h5', custom_objects={\"tf\": tf})\n",
    "            player_1_target_q.model = load_model('chase 1/player_1_target_q.h5', custom_objects={\"tf\": tf})\n",
    "            print(\"Player 1 Model Loaded\")\n",
    "        if train_player_2:\n",
    "            player_2_main_q.model = load_model('chase 1/player_2_main_q.h5', custom_objects={\"tf\": tf})\n",
    "            player_2_target_q.model = load_model('chase 1/player_2_target_q.h5', custom_objects={\"tf\": tf})\n",
    "            print(\"Player 2 model loaded\")\n",
    "        print(\"Models Loaded\")\n",
    "    except Exception:\n",
    "        if train_player_1:\n",
    "            player_1_main_q.model.save('chase 1/player_1_main_q.h5')\n",
    "            player_1_target_q.model.save('chase 1/player_1_target_q.h5')\n",
    "        if train_player_2:\n",
    "            player_2_main_q.model.save('chase 1/player_2_main_q.h5')\n",
    "            player_2_target_q.model.save('chase 1/player_2_target_q.h5')\n",
    "        print(\"New Models Created\")\n",
    "else:\n",
    "    if train_player_1:\n",
    "        player_1_target_q.model.set_weights(player_1_main_q.model.get_weights())\n",
    "    if train_player_2:\n",
    "        player_2_target_q.model.set_weights(player_2_main_q.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 2: 1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 3: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 4: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 5: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 6: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 7: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 8: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 9: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 10: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 11: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 12: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 13: 1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 14: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 15: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 16: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 17: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 18: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 19: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 20: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 21: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 22: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 23: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n",
      "Episode 24: -1, e is 1.00000\n",
      "player 1 losses over the last 10 steps: 0.00000\n",
      "player 2 losses over the last 10 steps: 0.00000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f87884f27d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m                                                  \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                                                  verbose=0)\n\u001b[1;32m--> 122\u001b[1;33m                             \u001b[0mplayer_1_batch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mtrain_player_2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                             \u001b[0mderived_actions2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer_2_main_q\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "# Set up some other avriables\n",
    "buffer = ExperienceBuffer(buffer_size = buffer_size)\n",
    "# player_2_buffer = ExperienceBuffer(buffer_size = buffer_size)\n",
    "total_steps = 0\n",
    "r_all = []\n",
    "e = start_e\n",
    "try:\n",
    "    ep = 0\n",
    "    while True:\n",
    "        ep += 1\n",
    "        ep_reward = 0\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            total_steps += 1\n",
    "            action1 = action2 = 4\n",
    "            if train_player_1:\n",
    "                if total_steps <= pre_train_steps or random() < e:\n",
    "                    action1 = randint(0,3)\n",
    "                else:\n",
    "                    action1 = player_1_main_q.act(np.reshape(s,\n",
    "                                                             [1,env.imsize,env.imsize,3]))[0]\n",
    "            if train_player_2:\n",
    "                if total_steps <= pre_train_steps or random() < e:\n",
    "                    action2 = randint(0,3)\n",
    "                else:\n",
    "                    action2 = player_2_main_q.act(np.reshape(s,\n",
    "                                                             [1,env.imsize,env.imsize,3]))[0]\n",
    "                \n",
    "            s1, r, done = env.step([action1, action2])\n",
    "            experience = [s,[action1, action2], r, s1, int(done)]\n",
    "            buffer.add(experience)\n",
    "            ep_reward += r\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > end_e:\n",
    "                    e -= step_drop\n",
    "                    e = max(e, end_e)\n",
    "                # can now start training\n",
    "                if total_steps % main_update_freq == 0:\n",
    "                    # update main q network\n",
    "                    # Get a sample of the past experiences\n",
    "                    player_1_batch_loss = 0\n",
    "                    player_2_batch_loss = 0\n",
    "\n",
    "                    if step_by_step_batch:\n",
    "                        batch = buffer.sample(batch_size)\n",
    "                        actual_batch_size = len(batch)\n",
    "                        for experience in batch:\n",
    "                            old_state = experience[0]\n",
    "                            action1 = int(experience[1][0])\n",
    "                            action2 = int(experience[1][1])\n",
    "                            reward1 = experience[2]\n",
    "                            reward2 = -experience[2]\n",
    "                            new_state = experience[3]\n",
    "                            ended = experience[4]\n",
    "                            if train_player_1:\n",
    "                                derived_action1 = player_1_main_q.act(np.reshape(new_state, \n",
    "                                                                                 [1,env.imsize,env.imsize,3]))[0]\n",
    "                                q_target1 = player_1_target_q.model.predict(np.reshape(new_state, \n",
    "                                                                                       [1,env.imsize,env.imsize,3]))[0]\n",
    "                                q_value1 = q_target1[int(derived_action1)]\n",
    "                                target1 = reward1 + discount*q_value1\n",
    "                                if ended:\n",
    "                                    target1 = reward1\n",
    "                                # print(target)\n",
    "                                # So need to feed in q_table not just updated values so update\n",
    "                                # batch q table with new values\n",
    "                                cur_q_values1 = player_1_main_q.model.predict(np.reshape(old_state, \n",
    "                                                                                         [1,env.imsize,env.imsize,3]))[0]\n",
    "                                cur_q_values1[action1] = target1\n",
    "                                loss_1 = player_1_main_q.model.fit(np.reshape(old_state, \n",
    "                                                                              [1,env.imsize,env.imsize,3]), \n",
    "                                                 np.reshape(cur_q_values1, [1, 4]), \n",
    "                                                 verbose=0)\n",
    "                                player_1_batch_loss += loss_1.history['loss'][0]/actual_batch_size\n",
    "\n",
    "                            if train_player_2:\n",
    "                                derived_action2 = player_2_main_q.act(np.reshape(new_state, \n",
    "                                                                                 [1,env.imsize,env.imsize,3]))[0]\n",
    "                                q_target2 = player_2_target_q.model.predict(np.reshape(new_state, \n",
    "                                                                                       [1,env.imsize,env.imsize,3]))[0]\n",
    "                                q_value2 = q_target2[int(derived_action2)]\n",
    "                                target2 = reward2 + discount*q_value2\n",
    "                                if ended:\n",
    "                                    target2 = reward2\n",
    "                                # print(target)\n",
    "                                # So need to feed in q_table not just updated values so update\n",
    "                                # batch q table with new values\n",
    "                                cur_q_values2 = player_2_main_q.model.predict(np.reshape(old_state, \n",
    "                                                                                         [1,env.imsize,env.imsize,3]))[0]\n",
    "                                cur_q_values2[action2] = target2\n",
    "                                loss_2 = player_2_main_q.model.fit(np.reshape(old_state, \n",
    "                                                                              [1,env.imsize,env.imsize,3]), \n",
    "                                                 np.reshape(cur_q_values2, [1, 4]), \n",
    "                                                 verbose=0)\n",
    "                                player_2_batch_loss += loss_2.history['loss'][0]/actual_batch_size\n",
    "                    else:\n",
    "                        # buffer_sample_size\n",
    "                        batch = buffer.sample(buffer_sample_size)\n",
    "                        actual_batch_size = len(batch)\n",
    "                        old_states = np.array([exp[0] for exp in batch])\n",
    "                        actions1 = np.array([exp[1][0] for exp in batch])\n",
    "                        actions2 = np.array([exp[1][1] for exp in batch])\n",
    "                        rewards1 = np.array([exp[2] for exp in batch])\n",
    "                        rewards2 = -np.array([exp[2] for exp in batch])\n",
    "                        new_states = np.array([exp[3] for exp in batch])\n",
    "                        ended = np.array([exp[4] for exp in batch])\n",
    "                        if train_player_1:\n",
    "                            derived_actions1 = player_1_main_q.act(new_states)\n",
    "                            q_targets1 = player_1_target_q.model.predict(new_states)\n",
    "                            q_values1 = q_targets1[range(actual_batch_size), derived_actions1]\n",
    "                            targets1 = rewards1 + discount*q_values1*(1-ended)\n",
    "                            # print(target)\n",
    "                            # So need to feed in q_table not just updated values so update\n",
    "                            # batch q table with new values\n",
    "                            cur_q_values1 = player_1_main_q.model.predict(old_states)\n",
    "                            cur_q_values1[range(actual_batch_size),actions1] = targets1\n",
    "                            loss_1 = player_1_main_q.model.fit(old_states, \n",
    "                                                 cur_q_values1,\n",
    "                                                 epochs=epochs,\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 verbose=0)\n",
    "                            player_1_batch_loss = sum(loss_1.history['loss'])/epochs\n",
    "                        if train_player_2:\n",
    "                            derived_actions2 = player_2_main_q.act(new_states)\n",
    "                            q_targets2 = player_2_target_q.model.predict(new_states)\n",
    "                            q_values2 = q_targets2[range(actual_batch_size), derived_actions2]\n",
    "                            targets2 = rewards2 + discount*q_values2*(1-ended)\n",
    "                            # print(target)\n",
    "                            # So need to feed in q_table not just updated values so update\n",
    "                            # batch q table with new values\n",
    "                            cur_q_values2 = player_2_main_q.model.predict(old_states)\n",
    "                            cur_q_values2[range(actual_batch_size),actions2] = targets2\n",
    "                            loss_2 = player_2_main_q.model.fit(old_states, \n",
    "                                                 cur_q_values2,\n",
    "                                                 epochs=epochs,\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 verbose=0)\n",
    "                            player_2_batch_loss = sum(loss_2.history['loss'])/epochs\n",
    "                    player_1_losses.append(player_1_batch_loss)\n",
    "                    player_2_losses.append(player_2_batch_loss)\n",
    "            s = s1\n",
    "            if done:\n",
    "                print(\"Episode {}: {}, e is {:.5f}\".format(ep, int(ep_reward), e))\n",
    "                if train_player_1:\n",
    "                    print(\"player 1 losses over the last 10 steps: {:.5f}\".format(\n",
    "                        sum(player_1_losses[-10:])/10))\n",
    "                if train_player_2:\n",
    "                    print(\"player 2 losses over the last 10 steps: {:.5f}\".format(\n",
    "                        sum(player_2_losses[-10:])/10))\n",
    "                if ep % target_update_freq == 0:\n",
    "                    # update target q network\n",
    "                    if train_player_1:\n",
    "                        main_weights = player_1_main_q.model.get_weights()\n",
    "                        weighted_main_weights = []\n",
    "                        for x in main_weights:\n",
    "                            weighted_main_weights.append(x*tau)\n",
    "                        target_weights = player_1_target_q.model.get_weights()\n",
    "                        weighted_target_weights = []\n",
    "                        for x in target_weights:\n",
    "                            weighted_target_weights.append(x*(1-tau))\n",
    "                        player_1_target_q.model.set_weights(weighted_main_weights + weighted_target_weights)\n",
    "                    \n",
    "                    if train_player_2:\n",
    "                        main_weights = player_2_main_q.model.get_weights()\n",
    "                        weighted_main_weights = []\n",
    "                        for x in main_weights:\n",
    "                            weighted_main_weights.append(x*tau)\n",
    "                        target_weights = player_2_target_q.model.get_weights()\n",
    "                        weighted_target_weights = []\n",
    "                        for x in target_weights:\n",
    "                            weighted_target_weights.append(x*(1-tau))\n",
    "                        player_2_target_q.model.set_weights(weighted_main_weights + weighted_target_weights)\n",
    "                    print()\n",
    "                    print(\"Updating Target Network\")\n",
    "                if ep % save_freq == 0 and save:\n",
    "                    print(\"Saving Networks\")\n",
    "                    if train_player_1:\n",
    "                        player_1_main_q.model.save('chase 1/player_1_main_q.h5')\n",
    "                        player_1_target_q.model.save('chase 1/player_1_target_q.h5')\n",
    "                    if train_player_2:    \n",
    "                        player_2_main_q.model.save('chase 1/player_2_main_q.h5')\n",
    "                        player_2_target_q.model.save('chase 1/player_2_target_q.h5')\n",
    "                break\n",
    "        r_all.append(ep_reward)\n",
    "except KeyboardInterrupt:\n",
    "    if save:\n",
    "        print(\"Saving Networks\")\n",
    "        if train_player_1:\n",
    "            player_1_main_q.model.save('chase 1/player_1_main_q.h5')\n",
    "            player_1_target_q.model.save('chase 1/player_1_target_q.h5')\n",
    "        if train_player_2:\n",
    "            player_2_main_q.model.save('chase 1/player_2_main_q.h5')\n",
    "            player_2_target_q.model.save('chase 1/player_2_target_q.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axs = fig.add_subplot(1,1,1)\n",
    "images = []\n",
    "done = False\n",
    "s = env.reset()\n",
    "im = axs.imshow(s, animated = True)\n",
    "images.append([im])\n",
    "\n",
    "while True:\n",
    "    action1 = action2 = 4\n",
    "    if train_player_1:\n",
    "        action1 = player_1_main_q.act(np.reshape(s,\n",
    "                                                 [1,env.imsize,env.imsize,3]))[0]\n",
    "    if train_player_2:\n",
    "        action2 = player_2_main_q.act(np.reshape(s,\n",
    "                                                 [1,env.imsize,env.imsize,3]))[0]\n",
    "    s, r, done = env.step([action1, action2])\n",
    "    im = axs.imshow(s, animated = True)\n",
    "    images.append([im])\n",
    "    if done == True:\n",
    "        break\n",
    "\n",
    "plt.ion()\n",
    "ani = animation.ArtistAnimation(fig, images)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
